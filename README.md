# Coding Assistant with DeepSeek LangChain
<img width="1459" alt="Screenshot 2025-03-30 at 15 11 26" src="https://github.com/user-attachments/assets/618fc51d-0ad3-4f09-88b1-a86348cd1c72" />


![Python](https://img.shields.io/badge/Python-3.9%2B-blue?logo=python&logoColor=white)
![Flask](https://img.shields.io/badge/Flask-000000?logo=flask&logoColor=white)
![Weaviate](https://img.shields.io/badge/Embeddings-Weaviate-orange?logo=weaviate)
![Llama](https://img.shields.io/badge/LLM-Ollama-yellow)
![License](https://img.shields.io/badge/License-MIT-green)


## Overview
This project is a **Streamlit-based AI Coding Assistant** powered by **DeepSeek LLM** and **LangChain**. It enables users to interact with an AI model for intelligent responses, leveraging LangChain's capabilities for enhanced conversational experiences.

## Features
- üß† Utilizes **DeepSeek LLM** for natural language processing
- üîó Built with **LangChain** for modular AI workflows
- üé® User-friendly **Streamlit UI** for seamless interaction
- üîÑ Supports contextual conversations and prompt engineering

## Demo
Run the app locally by following the setup instructions below.

## Installation
### Prerequisites
Ensure you have:
- Python 3.9+
- Ollama
- Required dependencies installed

### Setup
Clone the repository:
```bash
git clone https://github.com/Trinita21/deepseek-langchain-assistant.git
cd deepseek-langchain-assistant
```

Create and activate a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

Install dependencies:
```bash
pip install -r requirements.txt
```

Run the Streamlit app:
```bash
streamlit run app.py
```

## Usage
1. Enter your query or code snippet in the chat interface.
2. The AI Assistant, powered by DeepSeek and LangChain, will generate a response.
3. Experiment with different prompts to explore its capabilities.

## Technologies Used
- **DeepSeek LLM** - Core AI model
- **LangChain** - LLM-powered workflow framework
- **Streamlit** - Interactive web UI
- **Python** - Backend processing

---
‚≠ê If you find this project useful, consider starring the repo!

